{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESILIENT DISTRIBUTED DATASETS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To working with RDD use SparkContext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run transforamtions before the actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/27 08:38:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/03/27 08:38:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "conf = SparkConf().setMaster('local[*]').setAppName('spark-rdd-tut')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_test = sc.textFile('./data/sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 2 3 4 5', '3 4 5 66 77', '12 43 6 7 8', '12 12 33']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_test.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ITE00100554', 'EZE00100082'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station ITE00100554: -14.8째C\n",
      "Station EZE00100082: -13.5째C\n",
      "Station ITE00100554: 32.3째C\n",
      "Station EZE00100082: 32.3째C\n"
     ]
    }
   ],
   "source": [
    "def parse_line(line):\n",
    "    fields = line.split(',')\n",
    "    station_id = fields[0]\n",
    "    entry_type = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9/5.0) + 32.0\n",
    "    temperature = (temperature - 32) * 5/9\n",
    "\n",
    "    return(station_id, entry_type, temperature)\n",
    "\n",
    "lines = sc.textFile(\"./data/1800.csv\")\n",
    "parsed_lines = lines.map(parse_line)\n",
    "min_temps = parsed_lines.filter(lambda x: \"TMIN\" in x[1])\n",
    "max_temps = parsed_lines.filter(lambda x: \"TMAX\" in x[1])\n",
    "station_temps_min = min_temps.map(lambda x: (x[0], x[2]))\n",
    "station_temps_max = max_temps.map(lambda x: (x[0], x[2]))\n",
    "stations = station_temps_max.map(lambda x: (x[0]))\n",
    "print(set(stations.collect()))\n",
    "min_temps = station_temps_min.reduceByKey(lambda x,y: min(x,y))\n",
    "max_temps = station_temps_max.reduceByKey(lambda x,y: max(x,y))\n",
    "results_min = min_temps.collect()\n",
    "results_max = max_temps.collect()\n",
    "\n",
    "for result in results_min:\n",
    "    print(f\"Station {result[0]}: {round(result[1],2)}\\N{DEGREE SIGN}C\")\n",
    "\n",
    "for result in results_max:\n",
    "    print(f\"Station {result[0]}: {round(result[1],2)}\\N{DEGREE SIGN}C\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf = SparkConf().setMaster(\"local\").setAppName(\"WordCount\")\n",
    "#sc = SparkContext(conf=conf)\n",
    "\n",
    "input = sc.textFile('./data/book.txt')\n",
    "words = input.flatMap(lambda x: x.split())\n",
    "word_counts = words.countByValue()\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    clean_word = word.encode('ascii', 'ignore')\n",
    "    if (clean_word):\n",
    "        #print(clean_word, count)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_words(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "\n",
    "input = sc.textFile('./data/book.txt')\n",
    "words = input.flatMap(normalize_words)\n",
    "word_counts = words.countByValue()\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    clean_word = word.encode('ascii', 'ignore')\n",
    "    if clean_word:\n",
    "        #print(clean_word, count)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_words(text):\n",
    "    return re.compile(r'\\W+', re.UNICODE).split(text.lower())\n",
    "\n",
    "\n",
    "input = sc.textFile('./data/book.txt')\n",
    "words = input.flatMap(normalize_words)\n",
    "\n",
    "word_counts = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "word_counts_sorted = word_counts.map(lambda x: (x[1],x[0])).sortByKey()\n",
    "\n",
    "results = word_counts_sorted.collect()\n",
    "\n",
    "for result in results:\n",
    "    count = str(result[0])\n",
    "    word = result[1].encode('ascii', 'ignore')\n",
    "    if word:\n",
    "        #print(f'{word}: {count}')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 68 -> 6375.45$ \n",
      "User: 73 -> 6206.2$ \n",
      "User: 39 -> 6193.11$ \n",
      "User: 54 -> 6065.39$ \n",
      "User: 71 -> 5995.66$ \n",
      "User: 2 -> 5994.59$ \n"
     ]
    }
   ],
   "source": [
    "def parse_line(line):\n",
    "    fields = line.split(',')\n",
    "    user_id = fields[0]\n",
    "    price_type = float(fields[2])\n",
    "    \n",
    "    return(user_id, price_type)\n",
    "\n",
    "\n",
    "input_orders = sc.textFile('./data/customer-orders.csv')\n",
    "orders_by_users = input_orders.map(parse_line)\n",
    "\n",
    "orders_counts = orders_by_users.reduceByKey(lambda x, y: x + y)\n",
    "sorted_orders_by_users = orders_counts.map(lambda x: (x[1], x[0])).sortByKey(ascending=False)\n",
    "\n",
    "for i,s in enumerate(sorted_orders_by_users.collect()):\n",
    "    if i < 6:\n",
    "        formated = float(\"{:.2f}\".format(s[0]))\n",
    "        print(f\"User: {s[1]} -> {formated}$ \")\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATAFRAMES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SparkSession when creating a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, \n",
    "                               IntegerType, BinaryType,\n",
    "                              DateType, DoubleType,\n",
    "                            LongType, TimestampType)\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master('local[*]') \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo = spark.read.csv('./data/london_weather.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+----------------+--------+---------+--------+-------------+--------+----------+\n",
      "|    date|cloud_cover|sunshine|global_radiation|max_temp|mean_temp|min_temp|precipitation|pressure|snow_depth|\n",
      "+--------+-----------+--------+----------------+--------+---------+--------+-------------+--------+----------+\n",
      "|19790101|        2.0|     7.0|            52.0|     2.3|     -4.1|    -7.5|          0.4|101900.0|       9.0|\n",
      "|19790102|        6.0|     1.7|            27.0|     1.6|     -2.6|    -7.5|          0.0|102530.0|       8.0|\n",
      "|19790103|        5.0|     0.0|            13.0|     1.3|     -2.8|    -7.2|          0.0|102050.0|       4.0|\n",
      "|19790104|        8.0|     0.0|            13.0|    -0.3|     -2.6|    -6.5|          0.0|100840.0|       2.0|\n",
      "|19790105|        6.0|     2.0|            29.0|     5.6|     -0.8|    -1.4|          0.0|102250.0|       1.0|\n",
      "|19790106|        5.0|     3.8|            39.0|     8.3|     -0.5|    -6.6|          0.7|102780.0|       1.0|\n",
      "|19790107|        8.0|     0.0|            13.0|     8.5|      1.5|    -5.3|          5.2|102520.0|       0.0|\n",
      "|19790108|        8.0|     0.1|            15.0|     5.8|      6.9|     5.3|          0.8|101870.0|       0.0|\n",
      "|19790109|        4.0|     5.8|            50.0|     5.2|      3.7|     1.6|          7.2|101170.0|       0.0|\n",
      "|19790110|        7.0|     1.9|            30.0|     4.9|      3.3|     1.4|          2.1| 98700.0|       0.0|\n",
      "+--------+-----------+--------+----------------+--------+---------+--------+-------------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_wo.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_new = df_wo.withColumn('new_date', F.to_date(F.col(\"date\"), 'yyyyMMdd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_new = df_wo_new.drop('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wo_new = df_wo_new.filter(df_wo_new.new_date >= F.lit(\"2017-12-01\").cast(DateType())) \n",
    "df_wo_new = df_wo_new.filter(df_wo_new.new_date <= F.lit(\"2019-12-31\").cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+----------------+--------+---------+--------+-------------+--------+----------+----------+\n",
      "|cloud_cover|sunshine|global_radiation|max_temp|mean_temp|min_temp|precipitation|pressure|snow_depth|new_date  |\n",
      "+-----------+--------+----------------+--------+---------+--------+-------------+--------+----------+----------+\n",
      "|6.0        |3.5     |39.0            |7.5     |4.0      |0.8     |0.0          |102090.0|0.0       |2017-12-01|\n",
      "|8.0        |0.0     |13.0            |10.4    |5.0      |2.5     |0.4          |102710.0|0.0       |2017-12-02|\n",
      "|8.0        |0.0     |13.0            |10.0    |6.7      |3.0     |0.2          |102700.0|0.0       |2017-12-03|\n",
      "|7.0        |0.0     |13.0            |9.4     |7.0      |4.0     |0.0          |103330.0|0.0       |2017-12-04|\n",
      "|8.0        |0.0     |13.0            |11.6    |8.1      |6.8     |0.0          |103540.0|0.0       |2017-12-05|\n",
      "|8.0        |0.0     |13.0            |12.5    |9.7      |7.7     |0.2          |102730.0|0.0       |2017-12-06|\n",
      "|5.0        |0.9     |22.0            |4.6     |10.8     |9.0     |2.8          |101050.0|0.0       |2017-12-07|\n",
      "|2.0        |4.1     |39.0            |4.0     |3.5      |2.3     |0.0          |101580.0|0.0       |2017-12-08|\n",
      "|3.0        |3.7     |37.0            |2.6     |1.3      |-1.4    |12.0         |101510.0|0.0       |2017-12-09|\n",
      "|8.0        |0.0     |12.0            |3.5     |1.3      |0.0     |11.2         |98250.0 |1.0       |2017-12-10|\n",
      "|6.0        |0.0     |12.0            |8.0     |1.9      |0.4     |5.0          |98530.0 |0.0       |2017-12-11|\n",
      "|3.0        |4.4     |40.0            |9.7     |2.3      |-3.4    |3.2          |100720.0|0.0       |2017-12-12|\n",
      "|7.0        |0.1     |14.0            |6.9     |3.9      |-1.9    |7.2          |99660.0 |0.0       |2017-12-13|\n",
      "|2.0        |4.0     |38.0            |5.7     |4.8      |2.6     |1.4          |99130.0 |0.0       |2017-12-14|\n",
      "|4.0        |4.1     |38.0            |6.0     |3.3      |0.9     |0.0          |99870.0 |0.0       |2017-12-15|\n",
      "|3.0        |2.2     |29.0            |9.5     |3.0      |0.0     |0.2          |101780.0|0.0       |2017-12-16|\n",
      "|6.0        |0.0     |12.0            |7.9     |4.1      |-1.3    |3.2          |102280.0|0.0       |2017-12-17|\n",
      "|0.0        |4.7     |41.0            |8.4     |4.5      |1.0     |0.0          |102900.0|0.0       |2017-12-18|\n",
      "|5.0        |2.9     |32.0            |11.0    |3.0      |-2.3    |0.4          |103510.0|0.0       |2017-12-19|\n",
      "|8.0        |0.0     |12.0            |12.0    |6.0      |0.9     |0.0          |103520.0|0.0       |2017-12-20|\n",
      "+-----------+--------+----------------+--------+---------+--------+-------------+--------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_wo_new.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ID=21, name=\"b'Miles'\", age=19, num_friends=268)\n",
      "Row(ID=52, name=\"b'Beverly'\", age=19, num_friends=269)\n",
      "Row(ID=54, name=\"b'Brunt'\", age=19, num_friends=5)\n",
      "Row(ID=106, name=\"b'Beverly'\", age=18, num_friends=499)\n",
      "Row(ID=115, name=\"b'Dukat'\", age=18, num_friends=397)\n",
      "Row(ID=133, name=\"b'Quark'\", age=19, num_friends=265)\n",
      "Row(ID=136, name=\"b'Will'\", age=19, num_friends=335)\n",
      "Row(ID=225, name=\"b'Elim'\", age=19, num_friends=106)\n",
      "Row(ID=304, name=\"b'Will'\", age=19, num_friends=404)\n",
      "Row(ID=341, name=\"b'Data'\", age=18, num_friends=326)\n",
      "Row(ID=366, name=\"b'Keiko'\", age=19, num_friends=119)\n",
      "Row(ID=373, name=\"b'Quark'\", age=19, num_friends=272)\n",
      "Row(ID=377, name=\"b'Beverly'\", age=18, num_friends=418)\n",
      "Row(ID=404, name=\"b'Kasidy'\", age=18, num_friends=24)\n",
      "Row(ID=409, name=\"b'Nog'\", age=19, num_friends=267)\n",
      "Row(ID=439, name=\"b'Data'\", age=18, num_friends=417)\n",
      "Row(ID=444, name=\"b'Keiko'\", age=18, num_friends=472)\n",
      "Row(ID=492, name=\"b'Dukat'\", age=19, num_friends=36)\n",
      "Row(ID=494, name=\"b'Kasidy'\", age=18, num_friends=194)\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 18|    8|\n",
      "| 19|   11|\n",
      "| 20|    5|\n",
      "| 21|    8|\n",
      "| 22|    7|\n",
      "| 23|   10|\n",
      "| 24|    5|\n",
      "| 25|   11|\n",
      "| 26|   17|\n",
      "| 27|    8|\n",
      "| 28|   10|\n",
      "| 29|   12|\n",
      "| 30|   11|\n",
      "| 31|    8|\n",
      "| 32|   11|\n",
      "| 33|   12|\n",
      "| 34|    6|\n",
      "| 35|    8|\n",
      "| 36|   10|\n",
      "| 37|    9|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(ID=int(fields[0]), name=str(fields[1].encode(\"utf-8\")), \\\n",
    "               age=int(fields[2]), num_friends=int(fields[3]))\n",
    "\n",
    "lines = spark.sparkContext.textFile('./data/fakefriends.csv')\n",
    "people = lines.map(mapper)\n",
    "\n",
    "schema_people = spark.createDataFrame(people).cache()\n",
    "schema_people.createOrReplaceTempView('people')\n",
    "\n",
    "teenagers = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "for teen in teenagers.collect():\n",
    "    print(teen)\n",
    "\n",
    "schema_people.groupBy(\"age\").count().orderBy(\"age\").show()\n",
    "\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our inferred schema:\n",
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      "\n",
      "Let's display the name column:\n",
      "+--------+\n",
      "|    name|\n",
      "+--------+\n",
      "|    Will|\n",
      "|Jean-Luc|\n",
      "|    Hugh|\n",
      "|  Deanna|\n",
      "|   Quark|\n",
      "|  Weyoun|\n",
      "|  Gowron|\n",
      "|    Will|\n",
      "|  Jadzia|\n",
      "|    Hugh|\n",
      "|     Odo|\n",
      "|     Ben|\n",
      "|   Keiko|\n",
      "|Jean-Luc|\n",
      "|    Hugh|\n",
      "|     Rom|\n",
      "|  Weyoun|\n",
      "|     Odo|\n",
      "|Jean-Luc|\n",
      "|  Geordi|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Filter out anyone over 21:\n",
      "+------+-------+---+-------+\n",
      "|userID|   name|age|friends|\n",
      "+------+-------+---+-------+\n",
      "|    21|  Miles| 19|    268|\n",
      "|    48|    Nog| 20|      1|\n",
      "|    52|Beverly| 19|    269|\n",
      "|    54|  Brunt| 19|      5|\n",
      "|    60| Geordi| 20|    100|\n",
      "|    73|  Brunt| 20|    384|\n",
      "|   106|Beverly| 18|    499|\n",
      "|   115|  Dukat| 18|    397|\n",
      "|   133|  Quark| 19|    265|\n",
      "|   136|   Will| 19|    335|\n",
      "|   225|   Elim| 19|    106|\n",
      "|   304|   Will| 19|    404|\n",
      "|   327| Julian| 20|     63|\n",
      "|   341|   Data| 18|    326|\n",
      "|   349| Kasidy| 20|    277|\n",
      "|   366|  Keiko| 19|    119|\n",
      "|   373|  Quark| 19|    272|\n",
      "|   377|Beverly| 18|    418|\n",
      "|   404| Kasidy| 18|     24|\n",
      "|   409|    Nog| 19|    267|\n",
      "+------+-------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Group by age\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 31|    8|\n",
      "| 65|    5|\n",
      "| 53|    7|\n",
      "| 34|    6|\n",
      "| 28|   10|\n",
      "| 26|   17|\n",
      "| 27|    8|\n",
      "| 44|   12|\n",
      "| 22|    7|\n",
      "| 47|    9|\n",
      "| 52|   11|\n",
      "| 40|   17|\n",
      "| 20|    5|\n",
      "| 57|   12|\n",
      "| 54|   13|\n",
      "| 48|   10|\n",
      "| 19|   11|\n",
      "| 64|   12|\n",
      "| 41|    9|\n",
      "| 43|    7|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Make everyone 10 years older:\n",
      "+--------+----------+\n",
      "|    name|(age + 10)|\n",
      "+--------+----------+\n",
      "|    Will|        43|\n",
      "|Jean-Luc|        36|\n",
      "|    Hugh|        65|\n",
      "|  Deanna|        50|\n",
      "|   Quark|        78|\n",
      "|  Weyoun|        69|\n",
      "|  Gowron|        47|\n",
      "|    Will|        64|\n",
      "|  Jadzia|        48|\n",
      "|    Hugh|        37|\n",
      "|     Odo|        63|\n",
      "|     Ben|        67|\n",
      "|   Keiko|        64|\n",
      "|Jean-Luc|        66|\n",
      "|    Hugh|        53|\n",
      "|     Rom|        46|\n",
      "|  Weyoun|        32|\n",
      "|     Odo|        45|\n",
      "|Jean-Luc|        55|\n",
      "|  Geordi|        70|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "people = spark.read \\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .csv('./data/fakefriends-header.csv')\n",
    "\n",
    "print('Here is our inferred schema:')\n",
    "people.printSchema()\n",
    "\n",
    "print('Let\\'s display the name column:')\n",
    "people.select('name').show()\n",
    "\n",
    "print('Filter out anyone over 21:')\n",
    "people.filter(people.age < 21).show()\n",
    "\n",
    "print('Group by age')\n",
    "people.groupBy('age').count().show()\n",
    "\n",
    "print('Make everyone 10 years older:')\n",
    "people.select(people.name, people.age + 10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends = spark.read\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .csv('./data/fakefriends-header.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userID: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- friends: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|age|      avg(friends)|\n",
      "+---+------------------+\n",
      "| 18|           343.375|\n",
      "| 19|213.27272727272728|\n",
      "| 20|             165.0|\n",
      "| 21|           350.875|\n",
      "| 22|206.42857142857142|\n",
      "| 23|             246.3|\n",
      "| 24|             233.8|\n",
      "| 25|197.45454545454547|\n",
      "| 26|242.05882352941177|\n",
      "| 27|           228.125|\n",
      "| 28|             209.1|\n",
      "| 29|215.91666666666666|\n",
      "| 30| 235.8181818181818|\n",
      "| 31|            267.25|\n",
      "| 32| 207.9090909090909|\n",
      "| 33| 325.3333333333333|\n",
      "| 34|             245.5|\n",
      "| 35|           211.625|\n",
      "| 36|             246.6|\n",
      "| 37|249.33333333333334|\n",
      "+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends_by_age = friends.select('age', 'friends')\n",
    "\n",
    "friends_by_age.groupBy('age').avg('friends').sort('age').show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------+\n",
      "|age|round(avg(friends), 2)|\n",
      "+---+----------------------+\n",
      "| 18|                343.38|\n",
      "| 19|                213.27|\n",
      "| 20|                 165.0|\n",
      "| 21|                350.88|\n",
      "| 22|                206.43|\n",
      "| 23|                 246.3|\n",
      "| 24|                 233.8|\n",
      "| 25|                197.45|\n",
      "| 26|                242.06|\n",
      "| 27|                228.13|\n",
      "| 28|                 209.1|\n",
      "| 29|                215.92|\n",
      "| 30|                235.82|\n",
      "| 31|                267.25|\n",
      "| 32|                207.91|\n",
      "| 33|                325.33|\n",
      "| 34|                 245.5|\n",
      "| 35|                211.63|\n",
      "| 36|                 246.6|\n",
      "| 37|                249.33|\n",
      "+---+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends_by_age.groupBy('age')\\\n",
    "    .agg(F.round(F.avg('friends'),2))\\\n",
    "    .sort('age').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|age|friends_avg|\n",
      "+---+-----------+\n",
      "| 18|     343.38|\n",
      "| 19|     213.27|\n",
      "| 20|      165.0|\n",
      "| 21|     350.88|\n",
      "| 22|     206.43|\n",
      "| 23|      246.3|\n",
      "| 24|      233.8|\n",
      "| 25|     197.45|\n",
      "| 26|     242.06|\n",
      "| 27|     228.13|\n",
      "| 28|      209.1|\n",
      "| 29|     215.92|\n",
      "| 30|     235.82|\n",
      "| 31|     267.25|\n",
      "| 32|     207.91|\n",
      "| 33|     325.33|\n",
      "| 34|      245.5|\n",
      "| 35|     211.63|\n",
      "| 36|      246.6|\n",
      "| 37|     249.33|\n",
      "+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends_by_age.groupBy('age')\\\n",
    "    .agg(F.round(F.avg('friends'),2).alias('friends_avg'))\\\n",
    "    .sort('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = spark.read.text(\"./data/book.txt\")\n",
    "words = input_df.select(F.explode(F.split(input_df.value, '\\\\W+')).alias('word'))\n",
    "words.filter(words.word != '')\n",
    "\n",
    "lower_case_words = words.select(F.lower(words.word).alias('word'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_case_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
