{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/21 10:16:32 WARN Utils: Your hostname, ProBook resolves to a loopback address: 127.0.1.1; using 172.22.34.28 instead (on interface eth0)\n",
      "23/05/21 10:16:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/raddy/.pyenv/versions/3.10.10/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/raddy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/raddy/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2af07c2e-d509-4524-80d8-c4a7792f2acb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.1.0 in central\n",
      "\tfound io.delta#delta-storage;2.1.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.1 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.901 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.3.1!hadoop-aws.jar (111ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.901!aws-java-sdk-bundle.jar (23843ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.0.7.Final/wildfly-openssl-1.0.7.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;1.0.7.Final!wildfly-openssl.jar (57ms)\n",
      ":: resolution report :: resolve 4734ms :: artifacts dl 24022ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.1.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.1.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.1 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   7   |   3   |   3   |   0   ||   7   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2af07c2e-d509-4524-80d8-c4a7792f2acb\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 4 already retrieved (189078kB/210ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/21 10:17:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/21 10:17:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "myAccessKey = '' \n",
    "mySecretKey = ''\n",
    "\n",
    "def spark_session() -> SparkSession:\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = f\"\"\"--packages io.delta:delta-core_2.12:2.1.0,org.apache.hadoop:hadoop-aws:3.3.1 \n",
    "        --conf spark.hadoop.fs.s3a.access.key={myAccessKey} \\\n",
    "        --conf spark.hadoop.fs.s3a.secret.key={mySecretKey} pyspark-shell\"\"\"\n",
    "\n",
    "    builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config('spark.sql.session.timeZone', 'UTC')\n",
    "\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "    return spark\n",
    "\n",
    "\n",
    "spark = spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('./data/stocks.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+\n",
      "|symbol|      date|price|\n",
      "+------+----------+-----+\n",
      "|  MSFT|Jan 1 2000|39.81|\n",
      "|  MSFT|Feb 1 2000|36.35|\n",
      "|  MSFT|Mar 1 2000|43.22|\n",
      "|  MSFT|Apr 1 2000|28.37|\n",
      "|  MSFT|May 1 2000|25.45|\n",
      "|  MSFT|Jun 1 2000|32.54|\n",
      "|  MSFT|Jul 1 2000| 28.4|\n",
      "|  MSFT|Aug 1 2000| 28.4|\n",
      "|  MSFT|Sep 1 2000|24.53|\n",
      "|  MSFT|Oct 1 2000|28.02|\n",
      "|  MSFT|Nov 1 2000|23.34|\n",
      "|  MSFT|Dec 1 2000|17.65|\n",
      "|  MSFT|Jan 1 2001|24.84|\n",
      "|  MSFT|Feb 1 2001|   24|\n",
      "|  MSFT|Mar 1 2001|22.25|\n",
      "|  MSFT|Apr 1 2001|27.56|\n",
      "|  MSFT|May 1 2001|28.14|\n",
      "|  MSFT|Jun 1 2001| 29.7|\n",
      "|  MSFT|Jul 1 2001|26.93|\n",
      "|  MSFT|Aug 1 2001|23.21|\n",
      "+------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.format('delta').partitionBy('symbol').save('./data/delta/cars/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----+\n",
      "|symbol|      date|price|\n",
      "+------+----------+-----+\n",
      "|  AMZN|Jan 1 2000|64.56|\n",
      "|  AMZN|Feb 1 2000|68.87|\n",
      "|  AMZN|Mar 1 2000|   67|\n",
      "|  AMZN|Apr 1 2000|55.19|\n",
      "|  AMZN|May 1 2000|48.31|\n",
      "|  AMZN|Jun 1 2000|36.31|\n",
      "|  AMZN|Jul 1 2000|30.12|\n",
      "|  AMZN|Aug 1 2000| 41.5|\n",
      "|  AMZN|Sep 1 2000|38.44|\n",
      "|  AMZN|Oct 1 2000|36.62|\n",
      "|  AMZN|Nov 1 2000|24.69|\n",
      "|  AMZN|Dec 1 2000|15.56|\n",
      "|  AMZN|Jan 1 2001|17.31|\n",
      "|  AMZN|Feb 1 2001|10.19|\n",
      "|  AMZN|Mar 1 2001|10.23|\n",
      "|  AMZN|Apr 1 2001|15.78|\n",
      "|  AMZN|May 1 2001|16.69|\n",
      "|  AMZN|Jun 1 2001|14.15|\n",
      "|  AMZN|Jul 1 2001|12.49|\n",
      "|  AMZN|Aug 1 2001| 8.94|\n",
      "+------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM delta.`/home/raddy/projects/DataLab/ml-area/data/delta/cars/` WHERE symbol=\"AMZN\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = sc.textFile('./data/car-sales.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = cars.map( lambda el: el.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_toyota = cars.filter(lambda zoneRow: zoneRow[0] == 'Toyota')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Toyota', 'White', '150043', '4', '\"$4', '000.00\"'],\n",
       " ['Toyota', 'Blue', '32549', '3', '\"$7', '000.00\"'],\n",
       " ['Toyota', 'Green', '99213', '4', '\"$4', '500.00\"'],\n",
       " ['Toyota', 'White', '60000', '4', '\"$6', '250.00\"']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_toyota.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_rdd = cars.map(lambda car:(car[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Make', 1),\n",
       " ('Toyota', 1),\n",
       " ('Honda', 1),\n",
       " ('Toyota', 1),\n",
       " ('BMW', 1),\n",
       " ('Nissan', 1),\n",
       " ('Toyota', 1),\n",
       " ('Honda', 1),\n",
       " ('Honda', 1),\n",
       " ('Toyota', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_rdd = pair_rdd.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Make', 'BMW', 'Nissan', 'Honda']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_rdd.sortBy(keyfunc=lambda x: x[1]).keys().take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
